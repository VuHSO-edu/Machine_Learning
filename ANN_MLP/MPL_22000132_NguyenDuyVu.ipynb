{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI LAYER PERCEPTRON CHO BÀI TOÁN REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bai tap 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return np.mean((Yhat - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13592\\163357733.py:1: SyntaxWarning: invalid escape sequence '\\M'\n",
      "  path = 'D:\\Machine Learning\\ANN_MLP\\data\\SAT_GPA.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SAT</th>\n",
       "      <th>GPA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1714</td>\n",
       "      <td>2.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1664</td>\n",
       "      <td>2.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1760</td>\n",
       "      <td>2.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685</td>\n",
       "      <td>2.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1693</td>\n",
       "      <td>2.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    SAT   GPA\n",
       "0  1714  2.40\n",
       "1  1664  2.52\n",
       "2  1760  2.54\n",
       "3  1685  2.74\n",
       "4  1693  2.83"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'D:\\Machine Learning\\ANN_MLP\\data\\SAT_GPA.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"SAT\"]\n",
    "y = df[\"GPA\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=30/84, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy().reshape(-1, 1)\n",
    "X_test = X_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 17.104267\n",
      "iter 1000, loss: 0.201638\n",
      "iter 2000, loss: 0.199882\n",
      "iter 3000, loss: 0.199811\n",
      "iter 4000, loss: 0.199809\n",
      "iter 5000, loss: 0.199805\n",
      "iter 6000, loss: 0.199804\n",
      "iter 7000, loss: 0.199807\n",
      "iter 8000, loss: 0.199801\n",
      "iter 9000, loss: 0.199800\n",
      "\n",
      "Training Time: 0.810584 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.14674276, 3.04632643, 2.81725166, 3.09182758, 3.03377438,\n",
       "        3.215779  , 2.88628789, 2.71526631, 3.04789543, 2.65721312,\n",
       "        2.92394401, 3.07770653, 2.87530485, 3.0981036 , 2.96160014,\n",
       "        3.11065564, 2.82509668, 2.63995406, 2.70271427, 3.05103344,\n",
       "        2.80783762, 2.78116453, 2.85333878, 3.09496559, 2.69957626,\n",
       "        2.622695  , 2.79842359, 2.78900956, 2.97415218, 3.11850067,\n",
       "        2.91609899, 2.98356622, 2.66035113, 2.66976516, 3.18439889,\n",
       "        2.79528558, 2.82823469, 3.18282989, 2.79842359, 2.90354695,\n",
       "        2.77645752, 2.84078673, 2.87844286, 2.78116453, 2.80313061,\n",
       "        2.62426401, 2.76704349, 2.98356622, 2.63367804, 3.17498486,\n",
       "        2.91609899, 3.11222465, 2.73566338, 3.07927554]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predict Time: 0.000000 s\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "\n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: -1.4795317998992772\n",
      "MSE: 0.19696836006220456\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_train, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_train, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 21.095445\n",
      "iter 1000, loss: 0.185336\n",
      "iter 2000, loss: 0.201420\n",
      "iter 3000, loss: 0.201344\n",
      "iter 4000, loss: 0.197007\n",
      "iter 5000, loss: 0.198850\n",
      "iter 6000, loss: 0.199326\n",
      "iter 7000, loss: 0.199369\n",
      "iter 8000, loss: 0.199334\n",
      "iter 9000, loss: 0.199357\n",
      "\n",
      "Training Time: 1.005340 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_test.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_test, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_test.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_test.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "\n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.99142999, 3.33730941, 3.26257164, 3.20000049, 3.53892756,\n",
       "        3.48678494, 3.24345268, 3.06442966, 3.37554733, 2.98273955,\n",
       "        3.17914344, 3.21042902, 3.11831038, 3.19131005, 2.81588314,\n",
       "        3.16002448, 3.33730941, 3.11831038, 3.14090552, 2.9340731 ,\n",
       "        3.21738137, 3.03488217, 3.42942805, 3.18957197, 3.20347667,\n",
       "        3.22954798, 3.33209515, 3.29038104, 3.08007245, 3.43464231]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Predict Time: 0.000000 s\n"
     ]
    }
   ],
   "source": [
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    \n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "\n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: -0.5269318344583531\n",
      "MSE: 0.08242785224228712\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Time: 0.002528 s\n",
      "Average Predict Time: 0.000099 s\n",
      "R square: 0.05950572997060655\n",
      "MSE: 0.05077038868090673\n"
     ]
    }
   ],
   "source": [
    "linR = LinearRegression()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "linR.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(\"Training Time: %f s\" % training_time)\n",
    "\n",
    "\n",
    "num_runs = 10\n",
    "prediction_times = []\n",
    "\n",
    "for _ in range(num_runs):\n",
    "    start_time = time.time()\n",
    "    y_pred = linR.predict(X_test)\n",
    "    prediction_time = time.time() - start_time\n",
    "    prediction_times.append(prediction_time)\n",
    "    \n",
    "avg_prediction_time = np.mean(prediction_times)\n",
    "print(\"Average Predict Time: %f s\" % avg_prediction_time)\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 23.427248\n",
      "iter 1000, loss: 0.220017\n",
      "iter 2000, loss: 0.220260\n",
      "iter 3000, loss: 0.220303\n",
      "iter 4000, loss: 0.220310\n",
      "iter 5000, loss: 0.220310\n",
      "iter 6000, loss: 0.220309\n",
      "iter 7000, loss: 0.220308\n",
      "iter 8000, loss: 0.220307\n",
      "iter 9000, loss: 0.220305\n",
      "\n",
      "Training Time: 0.754600 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 75 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "\n",
    "     # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: -0.7796977051784229\n",
      "MSE: 0.09607282798607839\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 8.485891\n",
      "iter 1000, loss: 0.161161\n",
      "iter 2000, loss: 0.159679\n",
      "iter 3000, loss: 0.156982\n",
      "iter 4000, loss: 0.151789\n",
      "iter 5000, loss: 0.150593\n",
      "iter 6000, loss: 0.150436\n",
      "iter 7000, loss: 0.150417\n",
      "iter 8000, loss: 0.150415\n",
      "iter 9000, loss: 0.150416\n",
      "\n",
      "Training Time: 0.682132 s\n"
     ]
    }
   ],
   "source": [
    "d0 = 1\n",
    "d1 = h = 50 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0005 # learning rate\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "\n",
    "     # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\nTraining Time: %f s\" % training_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: -2.063773134785624\n",
      "MSE: 0.16539064387742072\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kết quả:\n",
    "- Loss giảm từ 17.1 xuống 0.199 trên tập train\n",
    "- Loss giảm từ 21.1 xuống 0.199 trên tập test\n",
    "- R-square âm (-1.48) cho thấy mô hình không fit tốt với dữ liệu\n",
    "- MSE khoảng 0.197 trên cả tập train và test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bai tap 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return np.mean((Yhat - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>TUOI</th>\n",
       "      <th>BMI</th>\n",
       "      <th>HA</th>\n",
       "      <th>GLUCOSE</th>\n",
       "      <th>CHOLESTEROL</th>\n",
       "      <th>BEDAYNTM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>21</td>\n",
       "      <td>160</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>18</td>\n",
       "      <td>150</td>\n",
       "      <td>12.0</td>\n",
       "      <td>4.97</td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>160</td>\n",
       "      <td>4.4</td>\n",
       "      <td>6.39</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>87</td>\n",
       "      <td>20</td>\n",
       "      <td>110</td>\n",
       "      <td>4.6</td>\n",
       "      <td>4.10</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  TUOI  BMI   HA  GLUCOSE  CHOLESTEROL  BEDAYNTM\n",
       "0   1    56   21  160     14.0         6.00      1.95\n",
       "1   2    76   18  150     12.0         4.97      1.33\n",
       "2   3    63   16  160      4.4         6.39      0.83\n",
       "3   4    78   20  100      4.0         7.00      2.00\n",
       "4   5    87   20  110      4.6         4.10      1.30"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'D:\\\\Machine Learning\\\\ANN_MLP\\\\data\\\\vidu4_lin_reg.txt'\n",
    "\n",
    "df = pd.read_csv(path, sep=\" \")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['ID', 'BEDAYNTM'], axis=1)\n",
    "y = df['BEDAYNTM']\n",
    "\n",
    "X_train = X[:80]\n",
    "y_train = y[:80]\n",
    "\n",
    "X_test = X[80:]\n",
    "y_test = y[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 1.633611\n",
      "iter 1000, loss: 0.151597\n",
      "iter 2000, loss: 0.151730\n",
      "iter 3000, loss: 0.151366\n",
      "iter 4000, loss: 0.151597\n",
      "iter 5000, loss: 0.151653\n",
      "iter 6000, loss: 0.151699\n",
      "iter 7000, loss: 0.151683\n",
      "iter 8000, loss: 0.151657\n",
      "iter 9000, loss: 0.151744\n",
      "Training done!\n"
     ]
    }
   ],
   "source": [
    "d0 = 5\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.001 # learning rate\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping (To avoid booming gradient)\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "\n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "print(\"Training done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.12050537, 1.24396561, 1.07393314, 1.08829821, 1.19939812,\n",
       "        1.17435056, 1.02479531, 1.0568369 , 1.17307447, 1.22543004,\n",
       "        0.67791447, 1.14196214, 1.21657454, 1.02297691, 0.9733986 ,\n",
       "        0.75010389, 1.07934872, 1.23229472, 1.0845406 , 1.06253063,\n",
       "        1.25021193, 1.1678294 , 1.36092958, 0.8112222 , 1.34817212,\n",
       "        1.2080271 , 0.99572062, 0.95518788, 1.25983538, 0.97087815,\n",
       "        1.01859184, 1.13043695, 1.13477176, 0.99869171, 0.85983173,\n",
       "        1.2021141 , 1.04086783, 1.06070685, 1.19347677, 1.11299237,\n",
       "        1.34518202, 1.03054771, 0.77596548, 0.9837873 , 1.0180409 ,\n",
       "        1.21911725, 0.93128345, 0.98178786, 1.01733744, 0.7579376 ,\n",
       "        1.01506994, 1.12206541, 1.02654034, 1.09602115, 1.2470717 ,\n",
       "        1.13005256, 0.79653497, 1.04417429, 0.76326444, 1.06952102,\n",
       "        0.98530297, 1.0671505 , 0.97772298, 1.08913447, 0.93789676,\n",
       "        0.88776262, 0.99944114, 1.08253036, 0.88229572, 1.0663376 ,\n",
       "        1.17059732, 1.4595678 , 1.12507892, 0.93471738, 0.75754251,\n",
       "        0.99821451, 1.02534718, 1.20774478, 1.12604075, 1.15070555]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: 0.1896031980110695\n",
      "MSE: 0.10452790454648944\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_train, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_train, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90317482, 0.80014592, 1.22404712, 0.94763144, 0.9499854 ,\n",
       "        1.04730146, 1.2112276 , 1.0507365 , 0.99744585, 0.69406933,\n",
       "        0.80628671, 0.75806581, 1.18092909, 1.06551095, 0.98182391,\n",
       "        1.0264556 , 1.1521737 , 1.18551   , 1.30530844, 1.28139224]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: 0.2110390308347614\n",
      "MSE: 0.22841425982569344\n"
     ]
    }
   ],
   "source": [
    "print(\"R square:\", r2_score(y_test, Z2[0]))\n",
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R square: 0.21453893938640334\n",
      "MSE: 0.22740099167615907\n"
     ]
    }
   ],
   "source": [
    "linR = LinearRegression()\n",
    "\n",
    "linR.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linR.predict(X_test)\n",
    "\n",
    "print(\"R square:\", r2_score(y_test, y_pred))\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kết quả:\n",
    "- R-square trên tập train đạt 0.19 và tập test đạt 0.21, cho thấy mô hình có khả năng dự đoán tốt hơn so với bài tập 1\n",
    "- MSE trên tập train là 0.10 và tập test là 0.23, thể hiện mô hình có độ chính xác khá tốt\n",
    "- Không có hiện tượng overfitting rõ ràng do hiệu suất trên tập test và train khá tương đồng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bai tap 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return np.mean((Yhat - Y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_13592\\3567143085.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  path = 'D:\\\\Machine Learning\\\\ANN_MLP\\data\\\\real_estate.csv'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>X1 transaction date</th>\n",
       "      <th>X2 house age</th>\n",
       "      <th>X3 distance to the nearest MRT station</th>\n",
       "      <th>X4 number of convenience stores</th>\n",
       "      <th>X5 latitude</th>\n",
       "      <th>X6 longitude</th>\n",
       "      <th>Y house price of unit area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012.917</td>\n",
       "      <td>32.0</td>\n",
       "      <td>84.87882</td>\n",
       "      <td>10</td>\n",
       "      <td>24.98298</td>\n",
       "      <td>121.54024</td>\n",
       "      <td>37.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012.917</td>\n",
       "      <td>19.5</td>\n",
       "      <td>306.59470</td>\n",
       "      <td>9</td>\n",
       "      <td>24.98034</td>\n",
       "      <td>121.53951</td>\n",
       "      <td>42.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013.583</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>47.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013.500</td>\n",
       "      <td>13.3</td>\n",
       "      <td>561.98450</td>\n",
       "      <td>5</td>\n",
       "      <td>24.98746</td>\n",
       "      <td>121.54391</td>\n",
       "      <td>54.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012.833</td>\n",
       "      <td>5.0</td>\n",
       "      <td>390.56840</td>\n",
       "      <td>5</td>\n",
       "      <td>24.97937</td>\n",
       "      <td>121.54245</td>\n",
       "      <td>43.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No  X1 transaction date  X2 house age  \\\n",
       "0   1             2012.917          32.0   \n",
       "1   2             2012.917          19.5   \n",
       "2   3             2013.583          13.3   \n",
       "3   4             2013.500          13.3   \n",
       "4   5             2012.833           5.0   \n",
       "\n",
       "   X3 distance to the nearest MRT station  X4 number of convenience stores  \\\n",
       "0                                84.87882                               10   \n",
       "1                               306.59470                                9   \n",
       "2                               561.98450                                5   \n",
       "3                               561.98450                                5   \n",
       "4                               390.56840                                5   \n",
       "\n",
       "   X5 latitude  X6 longitude  Y house price of unit area  \n",
       "0     24.98298     121.54024                        37.9  \n",
       "1     24.98034     121.53951                        42.2  \n",
       "2     24.98746     121.54391                        47.3  \n",
       "3     24.98746     121.54391                        54.8  \n",
       "4     24.97937     121.54245                        43.1  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'D:\\\\Machine Learning\\\\ANN_MLP\\data\\\\real_estate.csv'\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['No', 'Y house price of unit area'], axis=1)\n",
    "y = df['Y house price of unit area']\n",
    "\n",
    "X_train = X[:350]\n",
    "y_train = y[:350]\n",
    "\n",
    "X_test = X[350:]\n",
    "y_test = y[350:]\n",
    "\n",
    "y_train = y_train.to_numpy().reshape(-1, 1)\n",
    "y_test = y_test.to_numpy().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 1645.156576\n",
      "iter 1000, loss: 293.227067\n",
      "iter 2000, loss: 297.379269\n",
      "iter 3000, loss: 299.453560\n",
      "iter 4000, loss: 300.430561\n",
      "iter 5000, loss: 300.968239\n",
      "iter 6000, loss: 301.230568\n",
      "iter 7000, loss: 301.251704\n",
      "iter 8000, loss: 301.479591\n",
      "iter 9000, loss: 301.459114\n",
      "Training done!!!\n"
     ]
    }
   ],
   "source": [
    "d0 = 6\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 1\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "N = X_train.T.shape[1]\n",
    "eta = 0.0001 # learning rate\n",
    "\n",
    "for i in range(10000):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X_train.T) + b1\n",
    "    A1 = np.where(Z1 > 0, Z1, 0.01 * Z1)  # LeakyReLU\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = Z2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1000 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(y_train, Yhat)\n",
    "        print(\"iter %d, loss: %f\" %(i, loss))\n",
    "\n",
    "    # backpropagation\n",
    "    E2 = (Yhat - y_train.T)/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1 = np.where(Z1 > 0, E1, 0.01 * E1)  # Gradient of LeakyReLU\n",
    "    dW1 = np.dot(X_train.T, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Gradient clipping (To avoid booming gradient)\n",
    "    clip_value = 1.0\n",
    "    dW1 = np.clip(dW1, -clip_value, clip_value)\n",
    "    dW2 = np.clip(dW2, -clip_value, clip_value)\n",
    "    db1 = np.clip(db1, -clip_value, clip_value)\n",
    "    db2 = np.clip(db2, -clip_value, clip_value)\n",
    "    \n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "print(\"Training done!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[43.01756405, 27.08717658, 23.10195793, 27.17269877, 27.83695955,\n",
       "        47.59017742, 49.14957722, 49.8476156 , 52.28512235, 26.01553402,\n",
       "        46.98958241, 40.6959884 , 32.14508341, 46.69102302, 35.23460809,\n",
       "        24.51788373, 26.22103371, 26.05270577, 44.32851536, 24.15796331,\n",
       "        46.53869927, 48.98821524, 45.00753869, 50.41043014, 47.05566103,\n",
       "        27.16633922, 26.42965087, 54.62622889, 35.37681655, 50.50692873,\n",
       "        46.99874612, 52.13627831, 20.90712866, 38.69317942, 19.40776595,\n",
       "        50.70084672, 52.13880895, 25.11672307, 25.79466467, 44.10734486,\n",
       "        40.93287296, 27.28286785, 36.82053419, 32.18397802, 16.30481492,\n",
       "        40.52050686, 27.89747713, 29.40163501, 26.42965087, 49.4121477 ,\n",
       "        39.85596968, 26.45864039, 49.05129094, 45.8449472 , 46.42980619,\n",
       "        48.14854954, 48.49280638, 26.07939572, 24.50219292, 19.79925781,\n",
       "        53.57192873, 43.9253967 , 52.28811164, 53.37930671]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z1 = np.dot(W1.T, X_test.T) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "\n",
    "Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 52.64472422573521\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE:\", mean_squared_error(y_test, Z2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 65.18991450477287\n"
     ]
    }
   ],
   "source": [
    "linR = LinearRegression()\n",
    "\n",
    "linR.fit(X_train, y_train)\n",
    "\n",
    "y_pred = linR.predict(X_test)\n",
    "\n",
    "print(\"MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hiệu suất huấn luyện:\n",
    "- Mô hình MLP được huấn luyện trong 10000 vòng lặp với learning rate = 0.0001\n",
    "- Loss giảm đáng kể từ 1645.15 xuống còn 301.45, cho thấy mô hình đã hội tụ tốt\n",
    "- Việc sử dụng LeakyReLU và gradient clipping giúp tránh được vấn đề vanishing gradient\n",
    "\n",
    "2. So sánh với Linear Regression:\n",
    "- MLP cho kết quả MSE trên tập test là 52.64\n",
    "- Linear Regression cho kết quả MSE trên tập test là 65.18\n",
    "- MLP cho hiệu suất tốt hơn khoảng 19% so với Linear Regression\n",
    "\n",
    "3. Đánh giá tổng quan:\n",
    "- MLP thể hiện khả năng học tốt hơn các mối quan hệ phi tuyến trong dữ liệu bất động sản\n",
    "- Tuy nhiên, giá trị MSE vẫn còn khá cao, có thể cải thiện thêm bằng cách:\n",
    "  + Tăng số lượng hidden layer\n",
    "  + Điều chỉnh learning rate\n",
    "  + Thử nghiệm các hàm activation khác\n",
    "  + Áp dụng các kỹ thuật regularization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
